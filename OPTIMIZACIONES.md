# üöÄ Optimizaciones del Scraper

Este documento describe las optimizaciones implementadas para hacer el scraper m√°s eficiente y r√°pido.

## üìä Resumen de Mejoras

| Optimizaci√≥n | Beneficio | Speedup Estimado |
|-------------|-----------|------------------|
| **Paralelizaci√≥n** | Procesa 4 jugadores simult√°neamente | 3-4x m√°s r√°pido |
| **Session Pooling** | Reutiliza conexiones HTTP (keep-alive) | 20-30% m√°s r√°pido |
| **Cach√© de HTTP** | Evita requests duplicados a mismo URL | 100-1000x en hits |
| **Batch Saving** | Reduce escrituras a disco | 50-70% menos I/O |
| **Thread-safe** | Garantiza consistencia sin duplicados | - |
| **Delays optimizados** | Reducidos gracias a paralelizaci√≥n | 40% m√°s r√°pido |

**Speedup total estimado: 4-5x m√°s r√°pido** üéØ

---

## üîß Optimizaciones Implementadas

### 1. **Paralelizaci√≥n con ThreadPoolExecutor**

**Qu√© hace:**
- Procesa m√∫ltiples jugadores simult√°neamente (4 workers por defecto)
- Divide el scraping en 2 fases:
  - **Fase 1 (secuencial):** Recolecta datos b√°sicos de todas las p√°ginas
  - **Fase 2 (paralela):** Procesa perfiles completos en paralelo

**Configuraci√≥n:**
```python
# src/config/settings.py
self.MAX_WORKERS = 4  # N√∫mero de threads paralelos
```

**Beneficio:** Procesa 4 jugadores al mismo tiempo en vez de uno por uno.

---

### 2. **Session Pooling (HTTP Keep-Alive)**

**Qu√© hace:**
- Reutiliza la misma conexi√≥n TCP para m√∫ltiples requests
- Evita el overhead de crear nuevas conexiones

**Configuraci√≥n:**
```python
# src/config/settings.py
self.USE_SESSION_POOL = True  # Habilitar session pooling
```

**Beneficio:** Reduce latencia en ~20-30% al evitar handshakes TCP repetidos.

---

### 3. **Cach√© de HTTP Responses**

**Qu√© hace:**
- Cachea responses de requests HTTP
- Si se pide la misma URL, devuelve el resultado cacheado sin hacer request

**Uso:**
```python
# Con cach√© (default)
response = http_client.get(url, use_cache=True)

# Sin cach√© (para datos que cambian frecuentemente)
response = http_client.get(url, use_cache=False)
```

**Beneficio:** Speedup de 100-1000x cuando hay cache hits.

---

### 4. **Batch Saving**

**Qu√© hace:**
- Acumula jugadores en buffer y guarda cada N jugadores
- En vez de escribir a disco por cada jugador (lento), escribe en lotes

**Configuraci√≥n:**
```python
# src/config/settings.py
self.BATCH_SAVE_SIZE = 5  # Guardar cada 5 jugadores
```

**Beneficio:** Reduce operaciones de I/O en 50-70%.

---

### 5. **Thread-Safe con Locks**

**Qu√© hace:**
- Usa `threading.Lock()` para proteger escrituras concurrentes al JSON
- Garantiza que no haya race conditions ni duplicados

**Implementaci√≥n:**
- `StorageService` usa locks en `agregar_jugador()`
- Guardado at√≥mico del JSON con archivo temporal `.tmp`

**Beneficio:** Consistencia garantizada en ambiente paralelo.

---

### 6. **Delays Optimizados**

**Antes:**
```python
DELAY_ENTRE_JUGADORES = (0.5, 1.5)  # 0.5-1.5s
DELAY_ENTRE_PAGINAS = (2, 4)        # 2-4s
```

**Ahora:**
```python
DELAY_ENTRE_JUGADORES = (0.3, 0.8)  # 0.3-0.8s (reducido)
DELAY_ENTRE_PAGINAS = (1, 2)        # 1-2s (reducido)
```

**Por qu√© es seguro:**
- Al procesar en paralelo, los delays individuales son m√°s cortos
- El rate total sigue siendo aceptable (4 workers √ó 0.5s promedio = 2s por batch)

**Beneficio:** 40% menos tiempo de espera total.

---

## üéØ Configuraci√≥n Recomendada

### Para m√°xima velocidad (si Transfermarkt no bloquea):
```python
self.MAX_WORKERS = 6
self.BATCH_SAVE_SIZE = 10
self.DELAY_ENTRE_JUGADORES = (0.2, 0.5)
self.DELAY_ENTRE_PAGINAS = (0.5, 1)
```

### Para m√°xima seguridad (evitar bloqueos):
```python
self.MAX_WORKERS = 2
self.BATCH_SAVE_SIZE = 3
self.DELAY_ENTRE_JUGADORES = (0.5, 1.5)
self.DELAY_ENTRE_PAGINAS = (2, 3)
```

### Balance (configuraci√≥n actual):
```python
self.MAX_WORKERS = 4
self.BATCH_SAVE_SIZE = 5
self.DELAY_ENTRE_JUGADORES = (0.3, 0.8)
self.DELAY_ENTRE_PAGINAS = (1, 2)
```

---

## üìà Comparaci√≥n de Performance

### Antes de las optimizaciones:
```
500 jugadores √ó 3s promedio = 1500s = 25 minutos
```

### Despu√©s de las optimizaciones:
```
500 jugadores √∑ 4 workers √ó 1s promedio = 125s = ~2 minutos
```

**Speedup: 12x m√°s r√°pido** üöÄ

---

## üîí Garant√≠as de Consistencia

El scraper optimizado mantiene todas las garant√≠as de consistencia:

‚úÖ **No duplica jugadores:** Verifica existencia antes de procesar  
‚úÖ **No pierde datos:** Batch saving con flush al final  
‚úÖ **Thread-safe:** Locks en operaciones cr√≠ticas  
‚úÖ **Guardado at√≥mico:** Usa archivos temporales + rename  
‚úÖ **Manejo de errores:** Cada worker maneja sus propios errores  

---

## üõ†Ô∏è Debugging y Monitoreo

### Progress tracking mejorado:
```
  [  1/100] (  1.0%) [  3] Marco Ruben                   ‚úÖ
  [  2/100] (  2.0%) [  5] Alan Marinelli                ‚úÖ
  [  3/100] (  3.0%) [  7] Dami√°n Mart√≠nez               ‚ö†Ô∏è  Error
  [  4/100] (  4.0%) [  9] V√≠ctor Malcorra               ‚úÖ
```

### Mensajes de informaci√≥n:
- `‚ö° Paralelizaci√≥n: 4 workers` ‚Üí N√∫mero de threads
- `üíæ Batch saving: cada 5 jugadores` ‚Üí Frecuencia de guardado
- `üìã FASE 1: Recolectando...` ‚Üí Fase de recolecci√≥n
- `‚ö° FASE 2: Procesando en paralelo...` ‚Üí Fase de procesamiento

---

## üìù Notas Adicionales

1. **Cach√© se limpia autom√°ticamente** al finalizar el scraper
2. **Session se cierra autom√°ticamente** con `http_client.close()`
3. **Flush autom√°tico** al final garantiza que no se pierdan jugadores
4. **Delays aleatorios** (jitter) para parecer m√°s humano
5. **Backoff exponencial** en retries sigue activo

---

## üö¶ C√≥mo Usar

Simplemente ejecuta el scraper como siempre:

```bash
cd carc
source venv/bin/activate
python scripts/run_scraper.py
```

Las optimizaciones se aplican autom√°ticamente. No requiere cambios en tu workflow.

---

## üîÆ Futuras Optimizaciones Potenciales

Ideas para optimizar a√∫n m√°s (no implementadas):

1. **Async/await con asyncio:** Podr√≠a ser 2-3x m√°s r√°pido que threads
2. **Cach√© persistente en disco:** Mantener cach√© entre ejecuciones
3. **Distributed scraping:** M√∫ltiples m√°quinas en paralelo
4. **Rate limiter inteligente:** Ajustar delays basado en response times
5. **Compression:** Comprimir JSON para reducir tama√±o de archivo

---

**√öltima actualizaci√≥n:** 2026-02-27  
**Versi√≥n:** 2.1 (Optimizada + Minutos)
